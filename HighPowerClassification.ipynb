{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gibso\\AppData\\Local\\Temp\\ipykernel_12564\\3742390168.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import roughpy as rp\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "from FindAnomalousIntervals import find_anomalous_intervals\n",
    "from Utils import restrict_interval_for_power, create_power_sequence, compute_signature\n",
    "from Visualise import draw_segment\n",
    "from Augmentations import CumSum, AddTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anom = find_anomalous_intervals('NN',50,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_interval(before,after,row):\n",
    "    return row.signatures[max(0,row.anom_index+before):min(len(row.signatures), row.anom_index+after)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rifp = partial(restrict_interval,-5,15)\n",
    "anoms=df_anom[df_anom.channel.isin([5,6,10,11,13])].apply(rifp,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth=4\n",
    "find_sigs = partial(compute_signature,depth,[CumSum,AddTime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ps=anoms.apply(create_power_sequence)\n",
    "sigs=ps.apply(find_sigs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,roc_auc_score,roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class Scaler:\n",
    "    def __init__(self,thres=1e-10):\n",
    "        self.thres=thres\n",
    "    \n",
    "    def fit(self,X):\n",
    "        self.mu = X.mean(0)\n",
    "        X=X-self.mu\n",
    "        U, S, Vt = np.linalg.svd(X)\n",
    "        k = np.sum(S > self.thres)  # detected numerical rank\n",
    "        self.numerical_rank = k\n",
    "        self.Vt = Vt[:k]\n",
    "        self.S = S[:k]\n",
    "        \n",
    "    def transform(self,X):\n",
    "        x = X - self.mu\n",
    "        return x @ self.Vt.T  / self.S\n",
    "\n",
    "\n",
    "def EvaluateClassifier(X,y,split=0.4,n_neighbors=1,scal=True):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=split)\n",
    "\n",
    "    if scal:\n",
    "        scaler=Scaler(1e-8)\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    xgb_model=XGBClassifier()\n",
    "    logistic_regression = LogisticRegression()\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    xgb_model.fit(X_train,y_train)\n",
    "    xgb_pred=xgb_model.predict(X_test)\n",
    "\n",
    "    cr_xgb = classification_report(y_test, xgb_pred)\n",
    "\n",
    "\n",
    "    logistic_regression.fit(X_train, y_train)\n",
    "    lr_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "    cr_lr = classification_report(y_test, lr_pred)\n",
    "\n",
    "    knn.fit(X_train,y_train)\n",
    "    knn_pred = knn.predict(X_test)\n",
    "    cr_knn = classification_report(y_test, knn_pred)\n",
    "\n",
    "    return {'XGB':cr_xgb,'LR': cr_lr, 'KNN': cr_knn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [1.0, 10.0, 368.5799985403979, 3823.8880146676...\n",
       "0      [1.0, 10.0, 424.825157612431, 4461.18513515901...\n",
       "0      [1.0, 10.0, 96.78519380595532, 966.26652141278...\n",
       "1      [1.0, 10.0, 217.47215655114906, 1753.410843685...\n",
       "2      [1.0, 10.0, 213.79602083105937, 1759.812874668...\n",
       "                             ...                        \n",
       "387    [1.0, 10.0, 337.3512873084791, 2763.0627491806...\n",
       "53     [1.0, 10.0, 332.6937400490728, 2930.9164730513...\n",
       "402    [1.0, 10.0, 356.35523823228743, 2970.535257445...\n",
       "135    [1.0, 10.0, 585.6922391953755, 5676.9828230887...\n",
       "54     [1.0, 10.0, 285.3333121578959, 2373.9605698665...\n",
       "Length: 334, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask={5:0,6:0,10:1,11:2,13:3}\n",
    "results=EvaluateClassifier(np.vstack(sigs),df_anom[df_anom.channel.isin([5,6,10,11,13])].channel.replace(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95        50\n",
      "           1       0.91      0.89      0.90        47\n",
      "           2       0.96      0.96      0.96        26\n",
      "           3       0.82      0.82      0.82        11\n",
      "\n",
      "    accuracy                           0.93       134\n",
      "   macro avg       0.91      0.91      0.91       134\n",
      "weighted avg       0.93      0.93      0.93       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results['XGB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 5)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anom[df_anom.channel.isin([3])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, <pyximport._pyximport3.PyxImportMetaFinder at 0x1bfdec94b90>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyximport\n",
    "pyximport.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### As"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As using sig kernel talk about linear independence of signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sigkernel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_parameters = {'C': np.logspace(0, 4, 5), 'gamma': list(np.logspace(-4, 4, 9)) + ['auto']}\n",
    "_sigmas = [1e-3, 5e-3, 1e-2, 2.5e-2, 5e-2, 7.5e-2, 1e-1, 2.5e-1, 5e-1, 7.5e-1, 1., 2., 5., 10.]\n",
    "_scales = [5e-2, 1e-1, 5e-1, 1e0]\n",
    "trained_models = {}\n",
    "best_scores_train={'signature pde':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_anom[df_anom.channel.isin([5,6,10,11,13])].channel.replace(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seqs=[]\n",
    "y=[]\n",
    "for j in range(ps.shape[0]):\n",
    "    seq = ps.iloc[j]\n",
    "    if seq.shape[0] == 20:\n",
    "        seqs.append(seq)\n",
    "        y.append(labels.iloc[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack(seqs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask={5:0,6:0,10:1,11:2,13:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = LabelEncoder().fit_transform(y_train)\n",
    "y_test = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "# path-transform\n",
    "x_train = sigkernel.transform(X_train, at=True, ll=True, scale=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "if x_train.shape[0] <= 150 and x_train.shape[1] <=150 and x_train.shape[2] <= 8:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dtype = torch.float32\n",
    "else: # otherwise do computations in cython\n",
    "    device = 'cpu'\n",
    "    dtype = torch.float64\n",
    "\n",
    "# numpy -> torch\n",
    "x_train = torch.tensor(x_train, dtype=dtype, device=device)\n",
    "\n",
    "# grid search over sigmas\n",
    "sigmas = tqdm(_sigmas, position=2, leave=False)\n",
    "for sigma in sigmas:\n",
    "    sigmas.set_description(f\"signature PDE sigma: {sigma}\")\n",
    "\n",
    "    # define static kernel\n",
    "    static_kernel = sigkernel.RBFKernel(sigma=sigma)\n",
    "\n",
    "    # initialize corresponding signature PDE kernel\n",
    "    signature_kernel = sigkernel.SigKernel(static_kernel, dyadic_order=0)\n",
    "\n",
    "    # compute Gram matrix on train data\n",
    "    G_train = signature_kernel.compute_Gram(x_train, x_train, sym=True).cpu().numpy()\n",
    "\n",
    "    # SVC sklearn estimator\n",
    "    svc = SVC(kernel='precomputed', decision_function_shape='ovo')\n",
    "    svc_model = GridSearchCV(estimator=svc, param_grid=svc_parameters, cv=5, n_jobs=-1)\n",
    "    svc_model.fit(G_train, y_train)\n",
    "    \n",
    "    # empty memory\n",
    "    del G_train\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # store results\n",
    "    if svc_model.best_score_ > best_scores_train['signature pde']:\n",
    "        best_scores_train['signature pde'] = svc_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gibso\\AppData\\Local\\Temp\\ipykernel_12564\\39544720.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_train = torch.tensor(x_train, dtype=dtype, device=device)\n"
     ]
    }
   ],
   "source": [
    "x_test = sigkernel.transform(X_test, at=True, ll=True, scale=.1)\n",
    "\n",
    "# move to cuda (if available and memory doesn't exceed a certain threshold)\n",
    "if x_test.shape[0] <= 150 and x_test.shape[1] <=150 and x_test.shape[2] <= 10:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dtype = torch.float64\n",
    "else: # otherwise do computations in cython\n",
    "    device = 'cpu'\n",
    "    dtype = torch.float64\n",
    "    \n",
    "# numpy -> torch \n",
    "x_train = torch.tensor(x_train, dtype=dtype, device=device)\n",
    "x_test = torch.tensor(x_test, dtype=dtype, device=device)\n",
    "\n",
    "# define static kernel\n",
    "static_kernel = sigkernel.RBFKernel(sigma=sigma)\n",
    "\n",
    "# initialize corresponding signature PDE kernel\n",
    "signature_kernel = sigkernel.SigKernel(static_kernel, dyadic_order=0)\n",
    "    \n",
    "# compute Gram matrix on test data\n",
    "G_test = signature_kernel.compute_Gram(x_test, x_train, sym=False).cpu().numpy()\n",
    "\n",
    "# record scores\n",
    "train_score = svc_model.best_score_\n",
    "test_score = svc_model.score(G_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9384615384615385"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9384615384615385"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
